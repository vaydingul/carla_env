experiment_type: "eval_world_forward_model"
seed: 42
log_path: ~ #"/home/volkan/Documents/Codes/carla_env/logs/"

save_path: &save_path "figures/world_forward_model_detailed_evaluation/"

num_time_step_previous: &previous 20
num_time_step_future: &future 10
sequence_length: &sequence_length 30
num_workers: &num_workers 8
num_gpu: &num_gpu 1
read_keys: &read_keys ["bev_world"]
dilation: &dilation 1
bev_agent_channel: &bev_agent_channel 7
bev_vehicle_channel: &bev_vehicle_channel 6
bev_selected_channels: &bev_selected_channels [0, 1, 2, 3, 4, 5, 6, 11]
bev_calculate_offroad: &bev_calculate_offroad false

# Dataset related parameters for training

dataset_test:
  dilation: *dilation

# Dataloader related parameters for validation
dataloader_val:
  batch_size: 5000
  shuffle: false
  num_workers: *num_workers

# Training related parameters
training:
  num_epochs: 1000
  optimizer:
    type: "Adam"
    config:
      lr: 0.01
      weight_decay: 0.0001
  loss:
    criterion: "L1Loss"
  batch_size: 5
  shuffle: false
  num_workers: 0
  drop_last: true

# Training related parameters
evaluation:
  num_time_step_previous: *previous
  num_time_step_predict: *future
  sequence_length: *sequence_length
  metrics: [iou, accuracy, precision, recall, f1]
  test_step: 10
  thresholds: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
  wandb_log_interval: 1

  renderer:
    width: 1300
    height: 1000
    channel: 3
    background_color: [0, 0, 0]
    font_scale: 0.5
    font_color: [255, 255, 255]
    font_thickness: 1
    cursor: [0, 0]
    name: World Model Evaluation
    show: false
    save: true
    create_date_time_path: true
    save_path: *save_path

# Wandb related parameters
wandb:
  link: "vaydingul/mbl-refactored/gp4uxsmj"
  checkpoint_number: 5

  enable: true
  resume: false
  resume_checkpoint_number: 0
  project: "mbl-refactored"
  group: "eval-world-forward-model-multi-step-20Hz"
  name: "20-10-20Hz"
  id: ~
  notes: "20 Hz world forward model evaluation plots"
